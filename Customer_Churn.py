# -*- coding: utf-8 -*-
"""Bank Churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KoVjGOaoCB_lPB__OeCf4RPo0vd3wpna
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, KFold, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import FeatureUnion
from sklearn.metrics import roc_auc_score, precision_score, confusion_matrix
from scipy import stats
from sklearn.inspection import PartialDependenceDisplay
!pip install shap
import shap
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier

"""# New Section"""

data=pd.read_csv(r"/content/Bank Customer Churn Prediction.csv")

data.info()

data.head()

data.duplicated().sum()

data.describe()

data.columns

list=["gender","country","credit_card","products_number","active_member","churn"]
for i in list:
  print(data[i].value_counts())

plt.figure(figsize=(10,15))
data[['credit_score', 'age', 'tenure', 'balance', 'products_number', 'estimated_salary']].hist(rwidth=0.8)
plt.tight_layout()

fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 8))
sns.countplot(x='gender', hue='churn', data=data, ax=ax1)
sns.countplot(x='country', hue='churn', data=data, ax=ax2)

#box plot

"""
Point-biserial is specifically useful in situations where you want to measure the relationship between a binary outcome (like customer churn, presence of a disease, etc.) and a continuous predictor (like age, income, etc.).
Pearson is more general for exploring relationships between continuous variables."""

columns=['credit_score','age', 'tenure',
       'balance', 'products_number', 'credit_card', 'active_member',
       'estimated_salary', 'churn']
point_biserial_corr = {}

for col in columns:
   corr_value,p_value= stats.pointbiserialr(data[col],data["churn"])
   point_biserial_corr[col] = corr_value

sorted_corr=sorted(point_biserial_corr.items(),key=lambda x:x[1],reverse=True)

for i in sorted_corr:
 print(f"{i[0]}: {i[1]:.2f}")

"""Feature Enginnering


"""

data["bal_to_sal"]=data["balance"]/data["estimated_salary"]
data.head()

x=data.drop("churn",axis=1)
y=data["churn"]

from sklearn.model_selection import train_test_split

# Assuming X is your feature set and y is your target variable
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

class DataFrameSelector(BaseEstimator,TransformerMixin):
  def __init__(self,attribute_names):
    self.attribute_names=attribute_names
  def fit(self,X,y=None):
    return self
  def transform(self,X):
    return X[self.attribute_names]

num_pipeline=Pipeline([
    ("select_num",DataFrameSelector(['credit_score','age', 'tenure',
       'balance', 'products_number', 'credit_card', 'active_member',
       'estimated_salary'])),
    ("imputer",SimpleImputer(strategy="median")),
    ("scaler",StandardScaler())
                          ])

X_train_transformed = num_pipeline.fit_transform(X_train)
X_test_transformed = num_pipeline.transform(X_test)

cat_pipeline = Pipeline([
    ("select_cat", DataFrameSelector(['country', 'gender'])),
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(sparse_output=False))
])

X_train_cat = cat_pipeline.fit_transform(X_train)
X_test_cat = cat_pipeline.transform(X_test)

cats=cat_pipeline.named_steps["encoder"].categories_
cats

from sklearn.compose import ColumnTransformer

pre_process = ColumnTransformer(transformers=[
    ("num_pipeline", num_pipeline, ['credit_score', 'age', 'tenure', 'balance',
                                    'products_number', 'credit_card', 'active_member', 'estimated_salary']),
    ("cat_pipeline", cat_pipeline, ['country', 'gender'])
])

#x=data

#x_train=pre_proecess.fit_transform(x)

print("class distribution before smote:", np.bincount(y_train))

smote = SMOTE(random_state=28)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)

print("New class distribution after smote:", np.bincount(y_train_resampled))

kn = KNeighborsClassifier()
rf = RandomForestClassifier(random_state=4)
et = ExtraTreesClassifier(random_state=4)
ab = AdaBoostClassifier(random_state=4)
dt = DecisionTreeClassifier(random_state=4)
xg = XGBClassifier(random_state=4)
gbm = GradientBoostingClassifier(random_state=4)
nb = GaussianNB()
lr = LogisticRegression(random_state=4)
mlp = MLPClassifier(random_state=4)

kf=StratifiedKFold(n_splits=10,shuffle=True,random_state=4)

y_prob = cross_val_predict(kn, X_train_resampled, y_train_resampled, cv=10, method="predict_proba")
roc_auc = roc_auc_score(y_train_resampled, y_prob[:, 1])
print("roc_auc_score:", roc_auc)

y_prob=cross_val_predict(rf,X_train_resampled,y_train_resampled,cv=10,method="predict_proba")
roc_auc=roc_auc_score(y_train_resampled,y_prob[:,1])
print("roc_auc_score:",roc_auc)

y_proba = cross_val_predict(et, X_train_resampled, y_train_resampled, cv=10, method='predict_proba')
roc_auc = roc_auc_score(y_train_resampled, y_proba[:, 1])
print("Cross-Validation ROC AUC Score for XT:", roc_auc)

y_proba = cross_val_predict(ab, X_train_resampled, y_train_resampled, cv=10, method='predict_proba')
roc_auc = roc_auc_score(y_train_resampled, y_proba[:, 1])
print("Cross-Validation ROC AUC Score for AB:", roc_auc)

y_proba = cross_val_predict(dt, X_train_resampled, y_train_resampled, cv=10, method='predict_proba')
roc_auc = roc_auc_score(y_train_resampled, y_proba[:, 1])
print("Cross-Validation ROC AUC Score for DT:", roc_auc)

y_proba = cross_val_predict(xg, X_train_resampled, y_train_resampled, cv=10, method='predict_proba')
roc_auc = roc_auc_score(y_train_resampled, y_proba[:, 1])
print("Cross-Validation ROC AUC Score for XG:", roc_auc)

y_proba = cross_val_predict(gbm, X_train_resampled, y_train_resampled, cv=10, method='predict_proba')
roc_auc = roc_auc_score(y_train_resampled, y_proba[:, 1])
print("Cross-Validation ROC AUC Score for GBM:", roc_auc)

y_proba = cross_val_predict(nb, X_train_resampled, y_train_resampled, cv=10, method='predict_proba')
roc_auc = roc_auc_score(y_train_resampled, y_proba[:, 1])
print("Cross-Validation ROC AUC Score for NB:", roc_auc)

y_proba = cross_val_predict(lr, X_train_resampled, y_train_resampled, cv=10, method='predict_proba')
roc_auc = roc_auc_score(y_train_resampled, y_proba[:, 1])
print("Cross-Validation ROC AUC Score for LR:", roc_auc)

param_grid_rf = {
    "n_estimators": [50, 100],
    "max_depth": [None, 5],
    "random_state": [42]
}

param_grid_et = {
    "n_estimators": [50, 100],
    "max_depth": [None, 5],
    "random_state": [42]
}

param_grid_xg = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1],
    'random_state': [42]
}
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'p': [1, 2]  # 1: Manhattan distance, 2: Euclidean distance
}
param_grid_ada = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1.0],
    'algorithm': ['SAMME', 'SAMME.R']
}
param_grid_gb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'max_features': ['sqrt', 'log2', None]  # 'auto' is removed
}

# Create a dictionary with model names as keys and hyperparameter grids as values
models_params = {
    "RandomForest": (rf, param_grid_rf),
    "ExtraTree": (et, param_grid_et),
    "XGBoost": (xg, param_grid_xg),
    "KNeighbors": (kn, param_grid_knn),
    "AdaBoost": (ab, param_grid_ada),
    "GradientBoosting": (gbm, param_grid_gb)
}

from sklearn.model_selection import GridSearchCV

best_models = {}

for model_name, (model, param_grid) in models_params.items():
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
    grid_search.fit(X_train_resampled, y_train_resampled)
    best_models[model_name] = grid_search.best_estimator_
    print(f"Best parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best score for {model_name}: {grid_search.best_score_}")

fr = RandomForestClassifier(n_estimators = 100, max_depth = None, random_state = 42)
et = ExtraTreesClassifier(n_estimators = 100, max_depth = None, random_state = 42)
gb = GradientBoostingClassifier(
    learning_rate=0.2,  # Optimal learning rate
    max_depth=7,        # Optimal max depth
    max_features='sqrt',# Optimal max features
    n_estimators=200,   # Optimal number of estimators
    subsample=0.8,      # Optimal subsample ratio
    random_state=42     # Setting random state for reproducibility
)

classifiers = [fr, et, gb]

for classifier in classifiers:
    y_proba = cross_val_predict(estimator=classifier, X=X_train_resampled, y=y_train_resampled, cv=kf, method="predict_proba")

    # Compute ROC AUC score across all folds
    score = roc_auc_score(y_train_resampled, y_proba[:, 1])

    print(f"Classifier: {classifier}")
    print(f"ROC AUC score: {score}")
    print("\n")

X_t, X_h, y_t, y_h = train_test_split(X_train_resampled, y_train_resampled, test_size = 0.05, random_state = 42)

et.fit(X_t, y_t)

y_proba_train = et.predict_proba(X_t)
train_roc_auc = roc_auc_score(y_t, y_proba_train[:, 1])
train_precision = precision_score(y_t, et.predict(X_t))

y_proba_holdout = et.predict_proba(X_h)
holdout_roc_auc = roc_auc_score(y_h, y_proba_holdout[:, 1])
holdout_precision = precision_score(y_h, et.predict(X_h))

print(f"Train ROC AUC: {train_roc_auc}")
print(f"Train Precision: {train_precision}")
print(f"Holdout ROC AUC: {holdout_roc_auc}")
print(f"Holdout Precision: {holdout_precision}")

cm = confusion_matrix(y_h, et.predict(X_h))
print("Confusion Matrix:")
print(cm)

# Interpret the matrix
TN = cm[0, 0]
FP = cm[0, 1]
FN = cm[1, 0]
TP = cm[1, 1]

print("True Positives:", TP)
print("True Negatives:", TN)
print("False Positives:", FP)
print("False Negatives:", FN)

"""Implement Stacking and Blending"""

fitted_classifiers = []
for classifier in classifiers:
    classifier.fit(X_t, y_t)
    fitted_classifiers.append(classifier)

predictions_subset2 = []
for classifier in fitted_classifiers:
    predictions = classifier.predict(X_h)
    predictions_subset2.append(predictions)

#Create a feature matrix of all the predictions by the models
blending_feature_matrix = np.column_stack(predictions_subset2)

# Train a blending model (meta-model) on the blending feature matrix and true labels of subset2
blending_model = ExtraTreesClassifier(n_estimators = 100, max_depth = None, random_state = 42)
blending_model.fit(blending_feature_matrix, y_h)
blending_predictions = blending_model.predict(blending_feature_matrix)

blending_roc_auc = roc_auc_score(y_h, blending_predictions)
blending_precision = precision_score(y_h, blending_predictions)
print("ROC AUC Score for Blending Model:", blending_roc_auc)
print("Precision for Blending Model:", blending_precision)

"""**SHAP**"""

data.columns

features_importance = et.feature_importances_
feature_names = pd.Series(['credit_score', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary', 'France', 'Germany', 'Spain', 'Female', 'Male', 'bal_to_sal'])

features_importance = pd.Series(features_importance)
df = pd.concat([feature_names, features_importance], axis=1)
df

features_importance



